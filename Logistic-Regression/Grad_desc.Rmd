---
title: " Logistic Regression Gradiant Descent"
author: "Arnab Aich"
output: 
  html_document:
    toc: true
    highlight: tango
    code_folding: hide
    fig_width: 8
    fig_height: 6
    fig_align: "center"
    self_contained: true
    toc_depth: 3
    toc_float:
      collapsed: true
---

<link rel="stylesheet" type="text/css" href="../styles.css">

```{r include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA,
	echo = TRUE
)
invisible({lapply(c("readr","dplyr","stargazer","ggplot2","patchwork","kableExtra","pROC","caret","readsparse","here","Matrix","plotly"), require, character.only = TRUE)})
source(here("Logistic-Regression/Functions.R"))
```

# Introduction

In this document, we will implement a logistic regression model using gradient descent on **Gisette** and **Hill-Valley** dataset.
Gradient Descent is an optimization algorithm used to minimize a function by iteratively 
moving towards the direction of the steepest descent. It updates the model parameters 
in the following manner:

\[ \theta := \theta - \eta \nabla f(\theta) \]

Where:

- \( \theta \) are the parameters (weights) to be learned,
- \( \eta \) is the learning rate (step size),
- \( \nabla f(\theta) \) is the gradient of the cost function.

In the context of logistic regression, we use gradient descent to minimize the 
log-likelihood function, which is given by:

\[ L(w) = y \cdot Xw - \log(1 + e^{Xw}) \]

The goal is to update the weights iteratively until the log-likelihood converges.

\`\`\`{r include=FALSE}
# Here we load necessary libraries and datasets
\`\`\`

# Gradient Descent Algorithm

The gradient descent algorithm can be used to minimize the cost function for logistic 
regression. Below is the detailed implementation outline:

### Step 1: Initialization

We begin by initializing the model weights to zero or small random values, and set the 
learning rate (\( \eta \)) and regularization parameter (\( \lambda \)).

### Step 2: Calculating Predictions and Likelihood

For each iteration, we calculate the predicted values using the current weights and the 
features of the training data. The log-likelihood function is then evaluated.

### Step 3: Computing the Gradient

The gradient of the cost function with respect to the weights is computed. This gradient 
helps in updating the weights in the direction that minimizes the log-likelihood.

### Step 4: Updating the Weights

Using the computed gradient, we update the weights using the gradient descent rule:

\[ w := w - \eta \lambda w + (\eta / N) \times \nabla L(w) \]

Where \( N \) is the number of samples in the dataset.


### Step 5: Repeat Until Convergence

The process is repeated for a fixed number of iterations or until the change in 
log-likelihood is sufficiently small.


# Gissete Dataset
 
The Gisette dataset is a binary classification problem with 5000 features and 6000 training. At first we import the data.


```{r Data Import 1}
train_X <-
  read_table(here::here("Datasets/Gisette/gisette_train.data"), col_names = FALSE
  )
train_Y <-
  read_table(here::here("Datasets/Gisette/gisette_train.labels"), col_names = FALSE
  )
test_X <-
  read_table(here::here("Datasets/Gisette/gisette_valid.data"), col_names = FALSE
  )
test_Y <-
  read_table(here::here("Datasets/Gisette/gisette_valid.labels"), col_names = FALSE
  )
# Data Cleaning
test_Y[test_Y== -1] = 0
train_Y[train_Y== -1] = 0
x_mean=as.numeric(colMeans(train_X[,-5001]))
x_sd =as.numeric(apply(train_X[,-5001],2,sd))
X = rbind(scale(train_X[, -5001],center=x_mean,scale=x_sd),
          scale(test_X[, -5001],center=x_mean,scale=x_sd))
X = X[, colSums(is.na(X)) == 0]
X_train = X[1:6000, ]
X_train = cbind(X0 = rep(1, nrow(X_train)), X_train)
data_train = list(y = as.matrix(train_Y), x = as.matrix(X_train))
X_test = X[6001:7000, ]
X_test =  cbind(X0 = rep(1, nrow(X_test)), X_test)
data_test = list(y = as.matrix(test_Y), x = as.matrix(X_test))
```



```{r Computation}
r = logistic(300, data_train, data_test, eta = 0.001)

ggplotly(r$ROC.plot)

r$Table %>% as.data.frame() %>%
  kable(align = rep("c", ncol(r$Table))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")
```
# Hill-Valley Dataset

The Hill-Valley dataset is a binary classification problem with 100 features and 606 training samples.First we import the data. 

```{r Data Import 2,comment=NA, message=FALSE, results = FALSE}

train_X <- read_table(here("Datasets/hill-valley/X.dat"), col_names = FALSE)
x_mean= as.numeric(colMeans(train_X))
x_sd = as.numeric(apply(train_X,2,sd))
train_X1 <- cbind(rep(1,nrow(train_X)),scale(train_X,x_mean,x_sd))
train_Y <- read_table(here("Datasets/hill-valley/Y.dat"), col_names = FALSE)
data_train = list(x=as.matrix(train_X1),y=as.matrix(train_Y))
test_X <- read_table(here("Datasets/hill-valley/Xtest.dat"), col_names = FALSE)
test_X1 <- cbind(rep(1,nrow(test_X)),scale(test_X,x_mean,x_sd))
test_Y <- read_table(here("Datasets/hill-valley/Ytest.dat"), col_names = FALSE)
data_test = list(x=as.matrix(test_X1),y=as.matrix(test_Y))

```

Here we compute the results for the Hill-Valley dataset.

```{r Computing Results, message=FALSE, comment=NA}
r=logistic(500,data_train,data_test,eta=0.01)

ggplotly(r$ROC.plot)

r$Table %>% as.data.frame() %>%
  kable(align = rep("c", ncol(r$Table))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")
```
