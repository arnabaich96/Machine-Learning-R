---
title: "**Decision Tree and Random Forest**"
author: "**Arnab Aich**"
output:
  html_document:
    toc: true
    highlight: tango
    code_folding: hide
    fig_width: 8
    fig_height: 6
    fig_align: "center"
    self_contained: true
    toc_depth: 3
    toc_float:
      collapsed: true
---

<link rel="stylesheet" type="text/css" href="../styles.css">

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA,
	echo = TRUE
)
# Required libraries
lapply( c("readr", "caret", "ggplot2", "plotly", "parallel", "here", "doParallel", "rpart", "randomForest", "kableExtra", "knitr", "reshape2"),require ,character.only=TRUE)
set.seed(123)
```

# Introduction

In this document, we will explore the application of Decision Trees and Random Forests on the **Madelon** and **Satimage** datasets. These datasets are well-known in machine learning for their complexity and have been widely used in classification problems. We will walk through the steps of data preparation, model training, and evaluation of both algorithms, followed by a conclusion summarizing the findings.
```{r Load_data - Madelon}
# read and load Madelon Data
X <- read_table(here("Datasets","MADELON","madelon_train.data"),col_names = FALSE)
X=X[,-501]
Y <- read_table(here("Datasets","MADELON","madelon_train.labels"),col_names = FALSE)
x <- read_table(here("Datasets","MADELON","madelon_valid.data"),col_names = FALSE)
x=x[,-501]
y <- read_table(here("Datasets","MADELON","madelon_valid.labels"),col_names = FALSE)
mad_train = data.frame(Y,X)
mad_valid = data.frame(y,x)
# Read the Satimage data
sat_train <- read_table(here("Datasets","SATIMAGE","sat.trn"), col_names = FALSE)
sat_train <- as.data.frame(sat_train)
sat_test <- read_table(here("Datasets","SATIMAGE","sat.tst"), col_names = FALSE)
sat_test <- as.data.frame(sat_test)
```

#  Decision Tree Analysis

Decision trees are powerful and interpretable models used for both classification and regression tasks. In this section, we will implement decision trees on the **Madelon** and **Satimage** dataset and evaluate their performance. The goal is to determine how well decision trees can classify the data based on the selected features.
```{r decision_tree_madelon_train}
# Decision tree function
DT <- function(n, train, test) {
  # Fit the decision tree model
  fit <- rpart(X1 ~ ., data = as.data.frame(train), maxdepth = n, method = 'class')

  # Predict on test data
  pred <- predict(fit, as.data.frame(test), type = 'class')

  # Create confusion matrix
  confMat <- table(test[, 1], pred)

  # Calculate accuracy and misclassification
  accuracy <- round(sum(diag(confMat)) / sum(confMat), 3)
  miss <- 1 - accuracy

  # Return the tree size, depth, and performance metrics
  if (!is.null(fit$cptable)) {
    tree_size <- tail(fit$cptable[, "nsplit"], 1)  # Access nsplit as a matrix column
    min_test_error <- tail(fit$cptable[, "xerror"], 1)  # Access xerror as a matrix column
  } else {
    tree_size <- NA
    min_test_error <- NA
  }
data.frame(n, tree_size, min_test_error, accuracy, miss)

}
# set up parallel computing
my.cluster <- makeCluster(12)
registerDoParallel(my.cluster)
invisible(clusterEvalQ(my.cluster, {
  library(caret)
  library(rpart)
}))
clusterExport(my.cluster, c("DT","mad_train","mad_valid"))
num_tree <- seq(1, 12)
```
## Madelon Dataset

We will start by analyzing the **Madelon** dataset using decision trees. We will train the decision tree model on the training data and evaluate its performance on both the training and validation datasets. The analysis will include varying the tree depth to understand its impact on the model's accuracy and misclassification rate.

### Decision Tree Results - Training data:

We will train the decision tree model on the **Madelon** training data and evaluate its performance using different tree depths. The results will include the tree size, tree depth, minimum test error, accuracy, and misclassification rate for each tree depth.

```{r , results='hide' }
# Decision tree results for Madelon train
Result_train_madelon = t(parSapply(my.cluster, num_tree, function(n) {
  DT(n, train = mad_train, test = mad_train)
}))
colnames(Result_train_madelon) = c("Tree Size", "Tree Depth", "Min Test Error", "Accuracy", "Missclassification")
Result_train_madelon %>%
  kbl(align = rep("c", ncol(Result_train_madelon))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")
```

### Decision Tree Results - Validation data:

Next, we evaluate the decision tree model on the **Madelon** validation data to assess its generalization performance. We will use the same tree depths as in the training data and compare the results to understand how well the model performs on unseen data.

```{r}
# Decision tree results for Madelon validation
Result_valid_madelon = t(parSapply(my.cluster, num_tree, function(n) {
  DT(n, train = mad_train, test = mad_valid)
}))
stopCluster(my.cluster)

colnames(Result_valid_madelon) = c("Tree Size", "Tree Depth", "Min Test Error", "Accuracy", "Missclassification")

# Display result as table
Result_valid_madelon %>%
  kbl(align = rep("c", ncol(Result_train_madelon))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width = F, position = "center")
```

### Training and Validation Missclassifcation:

We can visualize the misclassification rates for the training and validation datasets across different tree depths. This comparison helps us understand how the model's performance changes with varying tree depths and provides insights into potential overfitting or underfitting.

```{r }
D = data.frame(Trees = num_tree,
          Train = unlist(Result_train_madelon[,"Missclassification"]),
          Valid = unlist(Result_valid_madelon[,"Missclassification"]))
# Plot the misclassification rate for train valid
# Convert data to long format using melt
D_melted <- melt(D, id.vars = "Trees")

# Plot using ggplot and ggplotly
ggplotly(
  ggplot(data = D_melted, aes(x = Trees, y = value, color = variable)) +
    geom_line() +
    geom_point() +  # Adds points for each value
    scale_x_continuous(breaks = D$Trees) +  # Ensures proper x-axis indexing based on Trees
    labs(title = "Misclassification Rate for Madelon Dataset",
         x = "Number of Trees",
         y = "Misclassification Rate") +
    scale_color_manual(values = c("Train" = "red", "Valid" = "blue")) +  # Custom legend labels
    guides(color = guide_legend(title = NULL)) +  # Removes the legend title (variable)
    theme_minimal() +
    theme(legend.position = "bottom")  # Move the legend to the bottom
)
```


```{r , results='hide' }
# set up parallel computing
my.cluster <- makeCluster(12)
registerDoParallel(my.cluster)
invisible(clusterEvalQ(my.cluster, {
  library(caret)
  library(rpart)
}))
clusterExport(my.cluster, c("DT","sat_train","sat_test"))
num_tree <- seq(1, 12)
```



## Satimage Dataset

Here, we apply decision trees to the **Satimage** dataset and evaluate their performance on both the training and validation data. The goal is to understand how well decision trees can classify the multi-spectral pixel values into different classes based on the selected features.


### Decision Tree Results - Training data:

We will train the decision tree model on the **Satimage** training data and evaluate its performance using different tree depths. The results will include the tree size, tree depth, minimum test error, accuracy, and misclassification rate for each tree depth.

```{r decision_tree_satimage_train , results='hide' }
# Decision tree results for satimage train
Result_train_satimage = t(parSapply(my.cluster, num_tree, function(n) {
  DT(n, train = sat_train, test = sat_train)
}))

colnames(Result_train_satimage) = c("Tree Size", "Tree Depth", "Min Test Error", "Accuracy", "Missclassification")
Result_train_satimage %>%
  kbl(align = rep("c", ncol(Result_train_satimage))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")


```


### Decision Tree Results - Validation data:

Next, we evaluate the decision tree model on the **Satimage** validation data to assess its generalization performance. We will use the same tree depths as in the training data and compare the results to understand how well the model performs on unseen data.

```{r decision_tree_satimage_valid}
# Decision tree results for satimage test
Result_test_satimage = t(parSapply(my.cluster, num_tree, function(n) {
  DT(n, train = sat_train, test = sat_test)
}))

stopCluster(my.cluster)

colnames(Result_test_satimage) = c("Tree Size", "Tree Depth", "Min Test Error", "Accuracy", "Missclassification")

# Display result as table
Result_test_satimage %>%
  kbl(align = rep("c", ncol(Result_test_satimage))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width = F, position = "center")
```


### Training and Validation Missclassifcation:

We can visualize the misclassification rates for the training and validation datasets across different tree depths. This comparison helps us understand how the model's performance changes with varying tree depths and provides insights into potential overfitting or underfitting.

```{r decision_tree_satimage_plot}

D = data.frame(Trees = num_tree,
          Train = unlist(Result_train_satimage[,"Missclassification"]),
          Valid = unlist(Result_test_satimage[,"Missclassification"]))
# Plot the misclassification rate for train valid
# Convert data to long format using melt
D_melted <- melt(D, id.vars = "Trees")

# Plot using ggplot and ggplotly
ggplotly(
  ggplot(data = D_melted, aes(x = Trees, y = value, color = variable)) +
    geom_line() +
    geom_point() +  # Adds points for each value
    scale_x_continuous(breaks = D$Trees) +  # Ensures proper x-axis indexing based on Trees
    labs(title = "Misclassification Rate - Satimage Dataset",
         x = "Number of Trees",
         y = "Misclassification Rate") +  
    theme_minimal()
)
```


# Random Forest Analysis

Random Forests are an extension of decision trees that improve accuracy and reduce overfitting by building multiple trees and averaging their results. In this section, we will build random forest models using different numbers of features and assess their performance on the **Madelon** dataset. We will also leverage parallel computing to speed up the training process.

```{r random_forest_madelon}
RF_out <- function(n_feature,train,test){
# number of trees
  k = c(3,10,30,100,300)
# Function to fit random forest model
  RF = function(n_tree,n_feature,train,test){
    test = as.data.frame(test)
    rf = randomForest(as.factor(X1)~., data = as.data.frame(train), mtry=n_feature,  ntree = n_tree, na.action = na.omit )
    pred  = predict(rf,test,type = "class")
    confMat <- table(test[,1],pred)
    accuracy = round(sum(diag(confMat))/sum(confMat),3)
    miss=1-accuracy
    return(miss)
  }
  miss_train_RF <- sapply(k,RF,n_feature,train,train)
  miss_test_RF <- sapply(k,RF,n_feature,train,test)
  output=data.frame(k,miss_train_RF,miss_test_RF)
  names(output) = c("Num_Tree","Train_Miss","Test_Miss")
  color = c("Train" = "red","Test" = "blue")
  p = ggplotly(
    ggplot(data = output, aes(x = Num_Tree, y = Train_Miss, color = "Train")) +
      geom_line() +
      geom_point() +
      geom_line(data = output, aes(x = Num_Tree, y = Test_Miss, color = "Test")) +
      geom_point(data = output, aes(x = Num_Tree, y = Test_Miss, color = "Test")) +
      scale_color_manual(values = color) +
      labs(title = "Random Forest: Train vs Test Missclassification",
           x = "Number of Trees",
           y = "Missclassification Rate") +
      theme_minimal()
  )
  return(list(plot = p,table=output))
}

# Number of features
n_feature = ceiling(c(log(500), sqrt(500), 500))
# Random forest results for Madelon
my.cluster <- makeCluster(3)
registerDoParallel(my.cluster)
invisible(clusterEvalQ(my.cluster, {
  library(caret)
  library(randomForest)
  library(ggplot2)
  library(plotly)
}))
clusterExport(my.cluster, c("RF_out","mad_train","mad_valid"))
RF_madelon = parLapply(my.cluster, n_feature, function(n_feat) {
  RF_out(n_feat, train = mad_train, test = mad_valid)
})
stopCluster(my.cluster)
```

### Number of feature $\approx log(500)$

In this subsection, we use the number of features approximately equal to `log(500)` for building the random forest. We will analyze how this specific number of features affects the model's classification performance and misclassification rate.

```{r random_forest_madelon_log}
RF_madelon[[1]]$table %>% kbl(align = rep("c", 3)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center") 

RF_madelon[[1]]$plot
```

### Number of feature \( \approx \sqrt{500} \) 

Here, we increase the number of features to approximately `sqrt(500)` and investigate the corresponding impact on the model’s accuracy and misclassification rate. This comparison helps understand the effect of feature selection on the overall model performance.

```{r random_forest_madelon_sqrt}
RF_madelon[[2]]$table %>% kbl(align = rep("c", 3)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center") 

RF_madelon[[2]]$plot
```

### Number of feature \( = 500 \)

In this case, we use all 500 features for training the random forest model. This approach helps us assess the model's performance when considering all available information without any dimensionality reduction.

```{r random_forest_madelon_full}
RF_madelon[[3]]$table %>% kbl(align = rep("c", 3)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center") 

RF_madelon[[3]]$plot
```

# Conclusion

We have analyzed the **Madelon** and **Satimage** datasets using decision trees and random forests. Both methods provide insight into the classification accuracy and the misclassification rates across different tree depths and random forest configurations. Proper parallel computing helps in speeding up the analysis significantly.
